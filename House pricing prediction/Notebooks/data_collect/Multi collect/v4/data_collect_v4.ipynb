{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import HTTPError\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def crawl_page(link=''):\n",
    "    # req = urllib.request.Request(\n",
    "    #         link, \n",
    "    #         data=None, \n",
    "    #         headers={\n",
    "    #             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "    #             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393'\n",
    "    #         }\n",
    "    # )\n",
    "    # try:\n",
    "    #     page= urllib.request.urlopen(req)\n",
    "    #     return_code= page.getcode()\n",
    "    #     soup_page= BeautifulSoup(page,'html.parser')\n",
    "    #     return {'page':soup_page, 'status': return_code}\n",
    "    # except HTTPError as err:\n",
    "    #     if err.code == 404:\n",
    "    #         return_code=404\n",
    "    #         return {'page':'no page', 'status': return_code}\n",
    "    #     else:\n",
    "    #         raise\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url=\"http://api.scraperapi.com\", #scrapeapi\n",
    "            params={\n",
    "                'api_key': '56fa8295856477d20660fa8c5dcd2638',\n",
    "                'url': link,  \n",
    "            },\n",
    "            \n",
    "        )\n",
    "        return_code= 200\n",
    "        soup_page= BeautifulSoup(response.content,'html.parser')\n",
    "        return {'page':soup_page, 'status': return_code}\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        if err.response.status_code == 404:\n",
    "            return_code=404\n",
    "            return {'page':'no page', 'status': return_code}\n",
    "    \n",
    "\n",
    "def write_csv(line=[], file=''):\n",
    "    with open(file, 'a', newline='') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=' ',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        spamwriter.writerow(line)\n",
    "\n",
    "def remove_row_csv():\n",
    "    with open('temp_27.csv') as inp:\n",
    "        for link in csv.reader(inp):\n",
    "            count=0\n",
    "            temp=link\n",
    "            with open('used_estate.txt') as out:\n",
    "                for estate in csv.reader(out):\n",
    "                    if re.match(\"(.*)/\"+estate[0],temp[1]) is not None:\n",
    "                        count+=1\n",
    "            if count==0:\n",
    "                new_row= [link[0]+','+link[1]]\n",
    "                write_csv(new_row,\"temp_28.csv\")\n",
    "\n",
    "def create_urls(file=''):\n",
    "    with open(file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 1\n",
    "        for row in csv_reader:\n",
    "            new_row= [str(line_count)+','+row[0]]\n",
    "            write_csv(new_row,'property_url_v4.csv')\n",
    "            line_count+=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollect():\n",
    "    def __init__(self):\n",
    "        self.__url_to_open='https://www.property24.com'\n",
    "        self.__soup=crawl_page(self.__url_to_open)['page']\n",
    "        self.__area_url=[]\n",
    "        self.__list_url=[]\n",
    "        self.__urls=pd.read_csv(\"temp_53_v4.csv\",usecols= ['id','url']) \n",
    "    \n",
    "    def collect_all_first_page_url(self):\n",
    "        other_links= self.__soup.find('div',class_='p24_popular').find(class_='col-8').find_all('a',class_='p24_bold')\n",
    "        for link in other_links:\n",
    "            self.__area_url.append(self.__url_to_open+link.get('href'))\n",
    "        first_element_tag= self.__soup.find('div',class_='p24_popular').find(class_='col-8').find_all(class_='row')[1]\n",
    "        first_link=first_element_tag.find('a').get('href')\n",
    "        temp_soup= crawl_page(self.__url_to_open+first_link)['page']\n",
    "        parent_link= temp_soup.find(id='breadCrumbContainer').find_all('a')[1]\n",
    "        self.__area_url.append(self.__url_to_open+parent_link.get('href'))\n",
    "    \n",
    "    def collect_all_url(self):\n",
    "        self.collect_all_first_page_url()\n",
    "        for link in self.__area_url:\n",
    "            soup= crawl_page(link)['page']\n",
    "            link_house_tag= soup.find('ul',class_='p24_relatedSales').find(href=re.compile(\"houses-for-sale\")).get('href')\n",
    "            link_apart_tag= soup.find('ul',class_='p24_relatedSales').find(href=re.compile(\"apartments-for-sale\")).get('href')\n",
    "            link_townhouse_tag= soup.find('ul',class_='p24_relatedSales').find(href=re.compile(\"townhouses-for-sale\")).get('href')\n",
    "            self.__list_url.append(link_house_tag)\n",
    "            self.__list_url.append(link_apart_tag)\n",
    "            self.__list_url.append(link_townhouse_tag)\n",
    "\n",
    "    def get_property_data(self, link=''):\n",
    "        data={}\n",
    "\n",
    "        estate_id= link.split('/')[-1].split('?')[0]\n",
    "\n",
    "        prop_soup_element= crawl_page(self.__url_to_open+link)\n",
    "\n",
    "        \n",
    "        if prop_soup_element['status'] !=200:\n",
    "            print('heres')\n",
    "            return False\n",
    "        \n",
    "        prop_soup= prop_soup_element['page']\n",
    "\n",
    "        if prop_soup is None:\n",
    "            print('here')\n",
    "            return False\n",
    "\n",
    "        #check if page not found\n",
    "        check_page_not_found_msg= prop_soup.select_one('.p24_not_found')\n",
    "        if check_page_not_found_msg is not None:\n",
    "            print(\"====> Page not found !!\")\n",
    "            return False\n",
    "        \n",
    "        #check page expired\n",
    "        check_page_expired = prop_soup.select_one('.p24_results.p24_expired')\n",
    "        if check_page_expired is not None:\n",
    "            print(\"====> Page expired !!\")\n",
    "            return False\n",
    "        \n",
    "\n",
    "        name= prop_soup.find('div',class_='p24_listingFeaturesWrapper').find('h1').get_text()\n",
    "        data['name']=name\n",
    "\n",
    "        price= prop_soup.find('div',class_='p24_listingFeaturesWrapper').find('div',class_=\"p24_price\").get_text()\n",
    "        data['price']=price\n",
    "\n",
    "        province= link.split('/')[4]\n",
    "        data['province']=province\n",
    "\n",
    "        city= link.split('/')[3]\n",
    "        data['city']=city\n",
    "\n",
    "\n",
    "        is_adress= len(prop_soup.find('div',class_='p24_listingFeaturesWrapper').find_all('div',class_=\"p24_mBM\")) >= 3\n",
    "        if is_adress:\n",
    "            if len(prop_soup.find('div',class_='p24_listingFeaturesWrapper').find_all('div',class_=\"p24_mBM\")) == 3:\n",
    "                index=-1\n",
    "            else:\n",
    "                index=3\n",
    "            address= prop_soup.find('div',class_='p24_listingFeaturesWrapper').find_all('div',class_=\"p24_mBM\")[index].get_text()\n",
    "            data['address']=address\n",
    "        else:\n",
    "            address=  prop_soup.select_one('.p24_addressPropOverview').get_text()\n",
    "            data['address']=address\n",
    "\n",
    "        #property overview\n",
    "        if type(prop_soup.select_one('.p24_propertyOverview')) is not None:\n",
    "            prop_overview_panel= prop_soup.find('div',class_='p24_propertyOverview').find('div',class_='panel-body')\n",
    "            prop_overview_row= prop_overview_panel.find_all('div',class_='p24_propertyOverviewRow')\n",
    "            for prop in prop_overview_row:\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\n",
    "                data[key]=value\n",
    "\n",
    "        #rooms\n",
    "        rooms_panel=prop_soup.select_one('#js_accordion_rooms')\n",
    "        if rooms_panel is not None:\n",
    "            prop_overview_row= rooms_panel.find_all('div',class_='p24_propertyOverviewRow')\n",
    "            for prop in prop_overview_row:\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\n",
    "                data[key]=value\n",
    "\n",
    "        #external_features\n",
    "        external_features_panel=prop_soup.select_one('#js_accordion_externalfeatures')\n",
    "        if external_features_panel is not None:\n",
    "            prop_overview_row= external_features_panel.find_all('div',class_='p24_propertyOverviewRow')\n",
    "            for prop in prop_overview_row:\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\n",
    "                data[key]=value\n",
    "        \n",
    "        #other_features\n",
    "        other_features_panel=prop_soup.select_one('#js_accordion_otherfeatures')\n",
    "        if other_features_panel is not None:\n",
    "            prop_overview_row= other_features_panel.find_all('div',class_='p24_propertyOverviewRow')\n",
    "            for prop in prop_overview_row:\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\n",
    "                data[key]=value\n",
    "\n",
    "        #building_features\n",
    "        building_features_panel=prop_soup.select_one('#js_accordion_building')\n",
    "        if building_features_panel is not None:\n",
    "            prop_overview_row= building_features_panel.find_all('div',class_='p24_propertyOverviewRow')\n",
    "            for prop in prop_overview_row:\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\n",
    "                data[key]=value\n",
    "\n",
    "        #points_interest\n",
    "        interest_soup= crawl_page(self.__url_to_open+'/ListingReadOnly/PointsOfInterestForListing?ListingNumber=P24-'+estate_id)\n",
    "        # points_interest_panel=prop_soup.select_one('#accordian-points-of-interest')\n",
    "        # print(points_interest_panel)\n",
    "        if interest_soup['status'] == 200:\n",
    "            categories= interest_soup['page'].select('.js_P24_POICategory')\n",
    "            for cat in categories:\n",
    "                interest= cat.select_one('.row > .col-6 > .p24_semibold')\n",
    "                distances= cat.select('.row > .col-6.noPadding.p24_semibold')\n",
    "                dist_tab=[]\n",
    "                for dist in distances:\n",
    "                    dist_tab.append(dist.get_text())\n",
    "                data[interest.get_text()]=dist_tab\n",
    "\n",
    "        print(str(data))\n",
    "        write_csv([str(data)],'data_property_v4.csv')\n",
    "\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def crawl_property_per_page(self,city,page,number_properties):\n",
    "        link=self.__url_to_open+city+'/p'+str(page)\n",
    "        print('==>Page link: ',link)\n",
    "        cur_soup= crawl_page(link)['page']\n",
    "        properties= cur_soup.find_all('div',class_='js_resultTile')\n",
    "        for prop_index in range(len(properties)-1):\n",
    "            prop_link= properties[prop_index].find('a').get('href')\n",
    "            write_csv([prop_link],'data_property_url.csv')\n",
    "\n",
    "        number_properties+= len(properties)\n",
    "        return number_properties\n",
    "\n",
    "    def crawl_property_per_city(self,city,cur_page):\n",
    "        soup= crawl_page(self.__url_to_open+city)['page']\n",
    "        pagination= soup.find('div',class_='p24_pager').find_all('li')[-1].find('a').get('data-pagenumber')\n",
    "        page_number= int(pagination)\n",
    "        number_properties=0\n",
    "        print(\"City url: \",city)\n",
    "        for page in range(cur_page, page_number+1):\n",
    "            print(\"=>Page number: \",page)\n",
    "            number_properties = self.crawl_property_per_page(city,page,number_properties)\n",
    "        print(number_properties)\n",
    "    \n",
    "    def crawl_properties(self):\n",
    "        #shuffle\n",
    "        self.__urls = self.__urls.sample(frac = 1).reset_index(drop=True)\n",
    "        for i in range(self.__urls.size):\n",
    "            prop_link= self.__urls.iloc[i]['url']\n",
    "            \n",
    "            print('==>Property link: ', prop_link)\n",
    "            check_property_crawl = self.get_property_data(prop_link)\n",
    "            if check_property_crawl:\n",
    "                self.__urls= self.__urls.drop(self.__urls.index[i])\n",
    "                self.__urls.to_csv('temp_54_v4.csv',index=False)\n",
    "\n",
    "    def crawl_property_url(self):\n",
    "        self.__list_url=['/townhouses-for-sale/gauteng/1']\n",
    "        for link in self.__list_url:\n",
    "            self.crawl_property_per_city(link,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Property link:  /for-sale/nelspruit-central/nelspruit/mpumalanga/2720/110117411\n",
      "==>Property link:  /for-sale/sonland-park/vereeniging/gauteng/3746/110043132\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-205986050037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d391c4dc02c6>\u001b[0m in \u001b[0;36mcrawl_properties\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==>Property link: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mcheck_property_crawl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_property_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_property_crawl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d391c4dc02c6>\u001b[0m in \u001b[0;36mget_property_data\u001b[0;34m(self, link)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprop_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p24_listingFeaturesWrapper'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-205986050037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d391c4dc02c6>\u001b[0m in \u001b[0;36mcrawl_properties\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==>Property link: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mcheck_property_crawl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_property_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_property_crawl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__urls\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d391c4dc02c6>\u001b[0m in \u001b[0;36mget_property_data\u001b[0;34m(self, link)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprop_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p24_listingFeaturesWrapper'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "dc= DataCollect()\n",
    "try:\n",
    "    dc.crawl_properties()\n",
    "except:\n",
    "    dc.crawl_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b54b66dfa7cdbe8a69b3bf148d9797a1fcee399928c1f348c50707ec239ca9b1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
