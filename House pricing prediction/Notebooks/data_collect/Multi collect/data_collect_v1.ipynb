{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "from urllib.error import HTTPError\r\n",
    "import urllib.request\r\n",
    "import requests\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "import csv\r\n",
    "\r\n",
    "def crawl_page(link=''):\r\n",
    "    # req = urllib.request.Request(\r\n",
    "    #         link, \r\n",
    "    #         data=None, \r\n",
    "    #         headers={\r\n",
    "    #             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\r\n",
    "    #             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393'\r\n",
    "    #         }\r\n",
    "    # )\r\n",
    "    # try:\r\n",
    "    #     page= urllib.request.urlopen(req)\r\n",
    "    #     return_code= page.getcode()\r\n",
    "    #     soup_page= BeautifulSoup(page,'html.parser')\r\n",
    "    #     return {'page':soup_page, 'status': return_code}\r\n",
    "    # except HTTPError as err:\r\n",
    "    #     if err.code == 404:\r\n",
    "    #         return_code=404\r\n",
    "    #         return {'page':'no page', 'status': return_code}\r\n",
    "    #     else:\r\n",
    "    #         raise\r\n",
    "    try:\r\n",
    "        response = requests.get(\r\n",
    "            url='http://api.scraperapi.com', #scrapeapi\r\n",
    "            params={\r\n",
    "                'api_key': 'edbfa8f8298eebfe56dca5fff89a39c6',\r\n",
    "                'url': link,  \r\n",
    "            },\r\n",
    "            \r\n",
    "        )\r\n",
    "        return_code= 200\r\n",
    "        soup_page= BeautifulSoup(response.content,'html.parser')\r\n",
    "        return {'page':soup_page, 'status': return_code}\r\n",
    "    except requests.exceptions.HTTPError as err:\r\n",
    "        if err.response.status_code == 404:\r\n",
    "            return_code=404\r\n",
    "            return {'page':'no page', 'status': return_code}\r\n",
    "    \r\n",
    "\r\n",
    "def write_csv(line=[], file=''):\r\n",
    "    with open(file, 'a', newline='') as csvfile:\r\n",
    "        spamwriter = csv.writer(csvfile, delimiter=' ',\r\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\r\n",
    "        spamwriter.writerow(line)\r\n",
    "\r\n",
    "def remove_row_csv():\r\n",
    "    with open('temp_27.csv') as inp:\r\n",
    "        for link in csv.reader(inp):\r\n",
    "            count=0\r\n",
    "            temp=link\r\n",
    "            with open('used_estate.txt') as out:\r\n",
    "                for estate in csv.reader(out):\r\n",
    "                    if re.match(\"(.*)/\"+estate[0],temp[1]) is not None:\r\n",
    "                        count+=1\r\n",
    "            if count==0:\r\n",
    "                new_row= [link[0]+','+link[1]]\r\n",
    "                write_csv(new_row,\"temp_28.csv\")\r\n",
    "\r\n",
    "def create_urls(file=''):\r\n",
    "    with open(file) as csv_file:\r\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\r\n",
    "        line_count = 1\r\n",
    "        for row in csv_reader:\r\n",
    "            new_row= [str(line_count)+','+row[0]]\r\n",
    "            write_csv(new_row,'property_url.csv')\r\n",
    "            line_count+=1\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class DataCollect():\r\n",
    "    def __init__(self):\r\n",
    "        self.__url_to_open='https://www.property24.com'\r\n",
    "        self.__soup=crawl_page(self.__url_to_open)['page']\r\n",
    "        self.__area_url=[]\r\n",
    "        self.__list_url=[]\r\n",
    "        self.__urls=pd.read_csv(\"temp_60_v1.csv\",usecols= ['id','url']) \r\n",
    "    \r\n",
    "    def collect_all_first_page_url(self):\r\n",
    "        other_links= self.__soup.find('div',class_='p24_popular').find(class_='col-8').find_all('a',class_='p24_bold')\r\n",
    "        for link in other_links:\r\n",
    "            self.__area_url.append(self.__url_to_open+link.get('href'))\r\n",
    "        first_element_tag= self.__soup.find('div',class_='p24_popular').find(class_='col-8').find_all(class_='row')[1]\r\n",
    "        first_link=first_element_tag.find('a').get('href')\r\n",
    "        temp_soup= crawl_page(self.__url_to_open+first_link)['page']\r\n",
    "        parent_link= temp_soup.find(id='breadCrumbContainer').find_all('a')[1]\r\n",
    "        self.__area_url.append(self.__url_to_open+parent_link.get('href'))\r\n",
    "    \r\n",
    "    def collect_all_url(self):\r\n",
    "        self.collect_all_first_page_url()\r\n",
    "        for link in self.__area_url:\r\n",
    "            soup= crawl_page(link)['page']\r\n",
    "            link_house_tag= soup.find('ul',class_='p24_relatedSales').find(href=re.compile(\"houses-for-sale\")).get('href')\r\n",
    "            link_apart_tag= soup.find('ul',class_='p24_relatedSales').find(href=re.compile(\"apartments-for-sale\")).get('href')\r\n",
    "            link_townhouse_tag= soup.find('ul',class_='p24_relatedSales').find(href=re.compile(\"townhouses-for-sale\")).get('href')\r\n",
    "            self.__list_url.append(link_house_tag)\r\n",
    "            self.__list_url.append(link_apart_tag)\r\n",
    "            self.__list_url.append(link_townhouse_tag)\r\n",
    "\r\n",
    "    def get_property_data(self, link=''):\r\n",
    "        data={}\r\n",
    "\r\n",
    "        estate_id= link.split('/')[-1].split('?')[0]\r\n",
    "\r\n",
    "        prop_soup_element= crawl_page(self.__url_to_open+link)\r\n",
    "\r\n",
    "        \r\n",
    "        if prop_soup_element['status'] !=200:\r\n",
    "            print('heres')\r\n",
    "            return False\r\n",
    "        \r\n",
    "        prop_soup= prop_soup_element['page']\r\n",
    "\r\n",
    "        if prop_soup is None:\r\n",
    "            print('here')\r\n",
    "            return False\r\n",
    "\r\n",
    "        #check if page not found\r\n",
    "        check_page_not_found_msg= prop_soup.select_one('.p24_not_found')\r\n",
    "        if check_page_not_found_msg is not None:\r\n",
    "            print(\"====> Page not found !!\")\r\n",
    "            return False\r\n",
    "        \r\n",
    "        #check page expired\r\n",
    "        check_page_expired = prop_soup.select_one('.p24_results.p24_expired')\r\n",
    "        if check_page_expired is not None:\r\n",
    "            print(\"====> Page expired !!\")\r\n",
    "            return False\r\n",
    "\r\n",
    "        if prop_soup.find('div',class_='p24_listingFeaturesWrapper') is None:\r\n",
    "            print(\"====> An error occured\")\r\n",
    "            return False\r\n",
    "        \r\n",
    "\r\n",
    "        name= prop_soup.find('div',class_='p24_listingFeaturesWrapper').find('h1').get_text()\r\n",
    "        data['name']=name\r\n",
    "\r\n",
    "        price= prop_soup.find('div',class_='p24_listingFeaturesWrapper').find('div',class_=\"p24_price\").get_text()\r\n",
    "        data['price']=price\r\n",
    "\r\n",
    "        province= link.split('/')[4]\r\n",
    "        data['province']=province\r\n",
    "\r\n",
    "        city= link.split('/')[3]\r\n",
    "        data['city']=city\r\n",
    "\r\n",
    "\r\n",
    "        is_adress= len(prop_soup.find('div',class_='p24_listingFeaturesWrapper').find_all('div',class_=\"p24_mBM\")) >= 3\r\n",
    "        if is_adress:\r\n",
    "            if len(prop_soup.find('div',class_='p24_listingFeaturesWrapper').find_all('div',class_=\"p24_mBM\")) == 3:\r\n",
    "                index=-1\r\n",
    "            else:\r\n",
    "                index=3\r\n",
    "            address= prop_soup.find('div',class_='p24_listingFeaturesWrapper').find_all('div',class_=\"p24_mBM\")[index].get_text()\r\n",
    "            data['address']=address\r\n",
    "        else:\r\n",
    "            address=  prop_soup.select_one('.p24_addressPropOverview').get_text()\r\n",
    "            data['address']=address\r\n",
    "\r\n",
    "        #property overview\r\n",
    "        if type(prop_soup.select_one('.p24_propertyOverview')) is not None:\r\n",
    "            prop_overview_panel= prop_soup.find('div',class_='p24_propertyOverview').find('div',class_='panel-body')\r\n",
    "            prop_overview_row= prop_overview_panel.find_all('div',class_='p24_propertyOverviewRow')\r\n",
    "            for prop in prop_overview_row:\r\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\r\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\r\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\r\n",
    "                data[key]=value\r\n",
    "\r\n",
    "        #rooms\r\n",
    "        rooms_panel=prop_soup.select_one('#js_accordion_rooms')\r\n",
    "        if rooms_panel is not None:\r\n",
    "            prop_overview_row= rooms_panel.find_all('div',class_='p24_propertyOverviewRow')\r\n",
    "            for prop in prop_overview_row:\r\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\r\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\r\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\r\n",
    "                data[key]=value\r\n",
    "\r\n",
    "        #external_features\r\n",
    "        external_features_panel=prop_soup.select_one('#js_accordion_externalfeatures')\r\n",
    "        if external_features_panel is not None:\r\n",
    "            prop_overview_row= external_features_panel.find_all('div',class_='p24_propertyOverviewRow')\r\n",
    "            for prop in prop_overview_row:\r\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\r\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\r\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\r\n",
    "                data[key]=value\r\n",
    "        \r\n",
    "        #other_features\r\n",
    "        other_features_panel=prop_soup.select_one('#js_accordion_otherfeatures')\r\n",
    "        if other_features_panel is not None:\r\n",
    "            prop_overview_row= other_features_panel.find_all('div',class_='p24_propertyOverviewRow')\r\n",
    "            for prop in prop_overview_row:\r\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\r\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\r\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\r\n",
    "                data[key]=value\r\n",
    "\r\n",
    "        #building_features\r\n",
    "        building_features_panel=prop_soup.select_one('#js_accordion_building')\r\n",
    "        if building_features_panel is not None:\r\n",
    "            prop_overview_row= building_features_panel.find_all('div',class_='p24_propertyOverviewRow')\r\n",
    "            for prop in prop_overview_row:\r\n",
    "                key= prop.find('div',class_='p24_propertyOverviewKey').get_text()\r\n",
    "                # value= prop.select_one('.noPadding > .p24_info').get_text()\r\n",
    "                value= prop.find('div',class_='noPadding').contents[1].get_text()\r\n",
    "                data[key]=value\r\n",
    "\r\n",
    "        #points_interest\r\n",
    "        interest_soup= crawl_page(self.__url_to_open+'/ListingReadOnly/PointsOfInterestForListing?ListingNumber=P24-'+estate_id)\r\n",
    "        # points_interest_panel=prop_soup.select_one('#accordian-points-of-interest')\r\n",
    "        # print(points_interest_panel)\r\n",
    "        if interest_soup['status'] == 200:\r\n",
    "            categories= interest_soup['page'].select('.js_P24_POICategory')\r\n",
    "            for cat in categories:\r\n",
    "                interest= cat.select_one('.row > .col-6 > .p24_semibold')\r\n",
    "                distances= cat.select('.row > .col-6.noPadding.p24_semibold')\r\n",
    "                dist_tab=[]\r\n",
    "                for dist in distances:\r\n",
    "                    dist_tab.append(dist.get_text())\r\n",
    "                data[interest.get_text()]=dist_tab\r\n",
    "\r\n",
    "        print(str(data))\r\n",
    "        write_csv([str(data)],'data_property.csv')\r\n",
    "\r\n",
    "        \r\n",
    "        return True\r\n",
    "\r\n",
    "    def crawl_property_per_page(self,city,page,number_properties):\r\n",
    "        link=self.__url_to_open+city+'/p'+str(page)\r\n",
    "        print('==>Page link: ',link)\r\n",
    "        cur_soup= crawl_page(link)['page']\r\n",
    "        properties= cur_soup.find_all('div',class_='js_resultTile')\r\n",
    "        for prop_index in range(len(properties)-1):\r\n",
    "            prop_link= properties[prop_index].find('a').get('href')\r\n",
    "            write_csv([prop_link],'data_property_url_v1.csv')\r\n",
    "\r\n",
    "        number_properties+= len(properties)\r\n",
    "        return number_properties\r\n",
    "\r\n",
    "    def crawl_property_per_city(self,city,cur_page):\r\n",
    "        soup= crawl_page(self.__url_to_open+city)['page']\r\n",
    "        pagination= soup.find('div',class_='p24_pager').find_all('li')[-1].find('a').get('data-pagenumber')\r\n",
    "        page_number= int(pagination)\r\n",
    "        number_properties=0\r\n",
    "        print(\"City url: \",city)\r\n",
    "        for page in range(cur_page, page_number+1):\r\n",
    "            print(\"=>Page number: \",page)\r\n",
    "            number_properties = self.crawl_property_per_page(city,page,number_properties)\r\n",
    "        print(number_properties)\r\n",
    "    \r\n",
    "    def crawl_properties(self):\r\n",
    "        #shuffle\r\n",
    "        self.__urls = self.__urls.sample(frac = 1).reset_index(drop=True)\r\n",
    "        for i in range(self.__urls.size):\r\n",
    "            prop_link= self.__urls.iloc[i]['url']\r\n",
    "            \r\n",
    "            print('==>Property link: ', prop_link)\r\n",
    "            check_property_crawl = self.get_property_data(prop_link)\r\n",
    "            if check_property_crawl:\r\n",
    "                self.__urls= self.__urls.drop(self.__urls.index[i])\r\n",
    "                self.__urls.to_csv('temp_61_v1.csv',index=False)\r\n",
    "\r\n",
    "    def crawl_property_url(self):\r\n",
    "        self.__list_url=['/townhouses-for-sale/gauteng/1']\r\n",
    "        for link in self.__list_url:\r\n",
    "            self.crawl_property_per_city(link,1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dc= DataCollect()\r\n",
    "try:\r\n",
    "    dc.crawl_properties()\r\n",
    "except:\r\n",
    "    dc.crawl_properties()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b54b66dfa7cdbe8a69b3bf148d9797a1fcee399928c1f348c50707ec239ca9b1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('deepai': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}